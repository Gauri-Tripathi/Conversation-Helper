# Model Configuration
model:
  base_model: "meta-llama/Llama-3.1-8B-Instruct"
  max_seq_length: 2048
  load_in_4bit: true
  use_flash_attention: true

# AdaLoRA Configuration
adalora:
  init_r: 16
  target_r: 8
  beta1: 0.85
  beta2: 0.85
  tinit: 200
  tfinal: 1000
  target_modules: 
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "down_proj"
    - "up_proj"
  lora_alpha: 16
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"

# Training Configuration
training:
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 8
  learning_rate: 2e-4
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.05
  num_train_epochs: 1
  weight_decay: 0.01
  fp16: true
  logging_steps: 50
  save_steps: 500
  eval_steps: 500
  save_total_limit: 3
  max_grad_norm: 0.3
  optim: "adamw_torch_fused"
  gradient_checkpointing: true

# Data Configuration
data:
  train_test_split: 0.1
  max_samples: null  # null for all samples
  seed: 42
  preprocessing:
    max_length: 512
    padding: "max_length"
    truncation: true
    return_tensors: "pt"

# Output Configuration
output:
  base_dir: "./outputs"
  model_dir: "final_model"
  log_dir: "logs"
  tensorboard_dir: "runs" 